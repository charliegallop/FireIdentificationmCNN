{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5275f9d4",
   "metadata": {},
   "source": [
    "# Transfer Learning using EfficientNetB4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f5404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c533c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir =pathlib.Path('/home/charlie/Documents/Uni/Exeter - Data Science/MTHM602_Trends_in_data_science_and_AI/Project/data/Training and Validation')\n",
    "test_dir =pathlib.Path('/home/charlie/Documents/Uni/Exeter - Data Science/MTHM602_Trends_in_data_science_and_AI/Project/data/Testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f486566",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire = list(data_dir.glob('fire/*'))\n",
    "img = PIL.Image.open(str(fire[0]))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337fab46",
   "metadata": {},
   "source": [
    "### Load data into a tf dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5bf1e0",
   "metadata": {},
   "source": [
    "Image input size: https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8956881",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "# Validation split 80/20\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir,\n",
    "                                                               validation_split = 0.2,\n",
    "                                                               subset = \"training\",\n",
    "                                                               shuffle = True,\n",
    "                                                               seed = 123,\n",
    "                                                               image_size = (img_height, img_width),\n",
    "                                                               batch_size = batch_size\n",
    "                                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf95709",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir,\n",
    "                                                             validation_split = 0.2,\n",
    "                                                             subset = \"validation\",\n",
    "                                                             shuffle = True,\n",
    "                                                             seed = 123,\n",
    "                                                             image_size = (img_height, img_width), \n",
    "                                                             batch_size = batch_size\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eeac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
    "                                                              shuffle = True,\n",
    "                                                              seed = 123,\n",
    "                                                              image_size = (img_height, img_width),    \n",
    "                                                              batch_size = batch_size\n",
    "                                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a9337",
   "metadata": {},
   "source": [
    "### Get Class Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982e2fd",
   "metadata": {},
   "source": [
    "### Visualise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8406f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d6dac",
   "metadata": {},
   "source": [
    "### Configure the dataset for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73585ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size = AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(buffer_size = AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(buffer_size = AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eacd4ec",
   "metadata": {},
   "source": [
    "### Create the base model from pre-trained convnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3848b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## By specifying \"include_top=False\" argument you load a network that doesn't include the classification layers at the top, which is ideal for feature extraction\n",
    "IMG_SHAPE = (img_width, img_height) + (3,)\n",
    "base_model = tf.keras.applications.EfficientNetB4(include_top=False,\n",
    "                                                  weights=\"imagenet\",\n",
    "                                                  input_shape=IMG_SHAPE,\n",
    "                                                  classifier_activation=\"softmax\"\n",
    "                                                 )\n",
    "image_batch, label_batch = next(iter(train_ds))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556cb85",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9565031",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a588b0",
   "metadata": {},
   "source": [
    "## Add classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d6dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731a52e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_layer = tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c14432",
   "metadata": {},
   "source": [
    "### Chain together the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434043f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape = (IMG_SHAPE))\n",
    "x = base_model(inputs, training = False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38854ac",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf891c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful metrics paper:# https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = base_learning_rate),\n",
    "              loss = tf.keras.losses.BinaryCrossentropy(from_logits=(True)),\n",
    "              metrics = [tf.keras.metrics.Precision(name = \"precision\"),\n",
    "                         tf.keras.metrics.Recall(name = \"recall\"),\n",
    "                         tf.keras.metrics.AUC(name = \"auc\")] \n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ecfa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69fcf2",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb8078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests the initial base model\n",
    "initial_epochs = 10\n",
    "loss0, precision0, recall0, auc0 = model.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23442254",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial precision: {:.2f}\".format(precision0))\n",
    "print(\"initial recall: {:.2f}\".format(recall0))\n",
    "print(\"initial auc: {:.2f}\".format(auc0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds,\n",
    "                    epochs = initial_epochs,\n",
    "                    validation_data = val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cf5555",
   "metadata": {},
   "source": [
    "### Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99220703",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = history.history['recall']\n",
    "val_recall = history.history['val_recall']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "auc = history.history['auc']\n",
    "val_auc = history.history['val_auc']\n",
    "\n",
    "precision = history.history['precision']\n",
    "val_pre = history.history['val_precision']\n",
    "\n",
    "plt.figure(figsize = (16, 16))\n",
    "plt.subplot(4, 2, 1)\n",
    "plt.plot(recall, label = 'Training Recall')\n",
    "plt.plot(val_recall, label='Validation Recall')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(4, 2, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.subplot(4, 2, 3)\n",
    "plt.plot(auc, label = 'Training AUC')\n",
    "plt.plot(val_auc, label='Validation AUC')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('AUC')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation AUC')\n",
    "plt.xlabel('epoch')\n",
    "plt.subplot(4, 2, 4)\n",
    "\n",
    "plt.plot(precision, label = 'Training Precision')\n",
    "plt.plot(val_pre, label='Validation Precision')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Precision')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6d61b",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc675c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previously the base model weights were not updated and only a few layers on top of the EfficientNetB4 was trained\n",
    "# Unfreeze and train the top layers of the pre-trained model to fine tune to the dataset\n",
    "\n",
    "## Un-freeze top layers of the model\n",
    "base_model.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e039116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 250\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b78e384",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Important to use a lower training weight since using more layers, otherwise\n",
    "# it could overfit very quickly \n",
    "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = True),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(learning_rate = base_learning_rate/10),\n",
    "              metrics = [tf.keras.metrics.Precision(name = \"precision\"),\n",
    "                         tf.keras.metrics.Recall(name = \"recall\"),\n",
    "                         tf.keras.metrics.AUC(name = \"auc\")\n",
    "                        ]\n",
    "             )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d543d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b6d3b",
   "metadata": {},
   "source": [
    "### Continue Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40db2745",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 10\n",
    "total_epochs = initial_epochs + fine_tune_epochs\n",
    "history_fine = model.fit(train_ds,\n",
    "                         epochs = total_epochs,\n",
    "                         initial_epoch = history.epoch[-1],\n",
    "                         validation_data = val_ds\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e306396",
   "metadata": {},
   "source": [
    "### Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf88e67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recall += history_fine.history['recall']\n",
    "val_recall += history_fine.history['val_recall']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']\n",
    "\n",
    "auc += history_fine.history['auc']\n",
    "val_auc += history_fine.history['val_auc']\n",
    "\n",
    "precision += history_fine.history['precision']\n",
    "val_pre += history_fine.history['val_precision']\n",
    "\n",
    "plt.figure(figsize = (16, 16))\n",
    "plt.subplot(4, 2, 1)\n",
    "plt.plot(recall, label = 'Training Recall')\n",
    "plt.plot(val_recall, label='Validation Recall')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.title('Training and Validation Recall')\n",
    "\n",
    "plt.subplot(4, 2, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.subplot(4, 2, 3)\n",
    "plt.plot(auc, label = 'Training AUC')\n",
    "plt.plot(val_auc, label='Validation AUC')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('AUC')\n",
    "plt.ylim([0.7,1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.title('Training and Validation AUC')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.subplot(4, 2, 4)\n",
    "plt.plot(precision, label = 'Training Precision')\n",
    "plt.plot(val_pre, label='Validation Precision')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.7,1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.title('Training and Validation Precision')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e96f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
    "                                                              shuffle = True,\n",
    "                                                              seed = 123,\n",
    "                                                              image_size = (img_height, img_width),\n",
    "                                                              batch_size = batch_size\n",
    "                                                             )\n",
    "test_ds = test_ds.prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060cebc0",
   "metadata": {},
   "source": [
    "## Verifying on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396da123",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, precision, recall, auc = model.evaluate(test_ds)\n",
    "print('Test loss :', loss)\n",
    "print('Test precision :', precision)\n",
    "print('Test recall :', recall)\n",
    "print('Test auc :', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a4318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a batch of images from the test set\n",
    "image_batch, label_batch = test_ds.as_numpy_iterator().next()\n",
    "predictions = model.predict_on_batch(image_batch).flatten()\n",
    "\n",
    "# pred = model.predict(test_ds)\n",
    "# pred = np.argmax(pred, axis = 1)[:5]\n",
    "#label = \n",
    "\n",
    "# Apply a sigmoid since our model returns logits\n",
    "#predictions = tf.nn.sigmoid(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955702b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(predictions, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41597eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.where(predictions < 0.5, 0, 1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a batch of images from the test set\n",
    "image_batch, label_batch = test_ds.as_numpy_iterator().next()\n",
    "predictions = model.predict_on_batch(image_batch).flatten()\n",
    "# pred = model.predict(test_ds)\n",
    "# pred = np.argmax(pred, axis = 1)[:5]\n",
    "# label = \n",
    "# Apply a sigmoid since our model returns logits\n",
    "#predictions = tf.nn.sigmoid(predictions)\n",
    "\n",
    "predictions = tf.where(predictions < 0.5, 0, 1)\n",
    "print('Predictions:', predictions.numpy())\n",
    "print('Labels:', label_batch)\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "    plt.title(class_names[predictions[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9039f",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c46f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model00_EfficientNetB4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469bd300",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b56e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import functions\n",
    "\n",
    "data_dir = pathlib.Path('/home/charlie/Documents/Uni/Exeter - Data Science/MTHM602_Trends_in_data_science_and_AI/Project/data/Training and Validation')\n",
    "test_dir = pathlib.Path('/home/charlie/Documents/Uni/Exeter - Data Science/MTHM602_Trends_in_data_science_and_AI/Project/data/Testing')\n",
    "loaded_model = tf.keras.models.load_model('models/model00_EfficientNetB4.h5')\n",
    "img_height, img_width, batch_size = [224, 224, 32]\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
    "                                                              shuffle = True,\n",
    "                                                              seed = 89787,\n",
    "                                                              image_size = (img_height, img_width),    \n",
    "                                                              batch_size = batch_size,\n",
    "                                                              labels = 'inferred',\n",
    "                                                              label_mode = 'binary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix, cm = functions.ConfusionMatrix(loaded_model, \n",
    "                                             test_ds, \n",
    "                                             save_fig = True, \n",
    "                                             save_fig_location = \"/home/charlie/Documents/Uni/Exeter - Data Science/MTHM602_Trends_in_data_science_and_AI/Project/plots/CM_NO_IA_1_EfficientNetB4.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313fad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.CreateMetricsReport(model = loaded_model,\n",
    "                             dataset = test_ds,\n",
    "                             confusion_matrix = confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a39c6f",
   "metadata": {},
   "source": [
    "## Testing on custom images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b96c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os\n",
    "import functions\n",
    "\n",
    "loaded_model = tf.keras.models.load_model('models/model00_EfficientNetB4.h5')\n",
    "input_shape = (224, 224)\n",
    "img_directory = '/home/charlie/Documents/Uni/Exeter - Data Science/MTHM602_Trends_in_data_science_and_AI/Project/data/Photos-001/'\n",
    "\n",
    "test_preds = functions.TestImages(img_directory, input_shape, loaded_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c2894",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in test_preds:\n",
    "    if i>0.5:\n",
    "        print(\"no fire\")\n",
    "    else:\n",
    "        count += 1\n",
    "        print(\"fire\")\n",
    "print('Incorrectly predicted ', count,  ' out of ',len(test_preds), '. FPR: ', round(count/len(test_preds), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996310b1",
   "metadata": {},
   "source": [
    "## Testing on other dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d82d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 10:38:14.271334: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-26 10:38:14.271351: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-04-26 10:38:17.076980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-26 10:38:17.077159: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/charlie/miniconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-26 10:38:17.077216: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/charlie/miniconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-26 10:38:17.077260: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/charlie/miniconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-26 10:38:17.077308: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/charlie/miniconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-26 10:38:17.077346: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/charlie/miniconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-26 10:38:17.077384: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/charlie/miniconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-26 10:38:17.077420: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/charlie/miniconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-26 10:38:17.077457: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/charlie/miniconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-26 10:38:17.077464: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-26 10:38:17.077870: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2204 files belonging to 2 classes.\n",
      "processing class directory  NoFire\n",
      "processing class directory  Fire\n",
      " no improper image files were found\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import functions\n",
    "\n",
    "data_dir = '../../data/Alt_dataset/Training Dataset'\n",
    "loaded_model = tf.keras.models.load_model('models/model00_EfficientNetB4.h5')\n",
    "\n",
    "img_height, img_width, batch_size = [224, 224, 32] \n",
    "\n",
    "\n",
    "custom_test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    shuffle = True,\n",
    "    seed = 89787,\n",
    "    image_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    labels = 'inferred',\n",
    "    label_mode = 'binary'\n",
    "    )\n",
    "\n",
    "## Check if images are in the right format\n",
    "\n",
    "bad_file_list = functions.CheckImagesReport(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005c8183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No incompatible files\n"
     ]
    }
   ],
   "source": [
    "## Delete the incompatible files from the folder\n",
    "if bad_file_list:\n",
    "    functions.DeleteIncompatibleImages(bad_file_list)\n",
    "else:\n",
    "    print(\"No incompatible files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e2cb656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "figure has been saved to: /home/charlie/Documents/Uni/Exeter - Data Science/MTHM602_Trends_in_data_science_and_AI/Project/plots/CM_NO_IA_2_EfficientNetB4.svg\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix, cm = functions.ConfusionMatrix(loaded_model, \n",
    "                                             custom_test_ds, \n",
    "                                             save_fig = True, \n",
    "                                             save_fig_location = \"/home/charlie/Documents/Uni/Exeter - Data Science/MTHM602_Trends_in_data_science_and_AI/Project/plots/EfficientNetB4/CM_NO_IA_2_EfficientNetB4.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da701e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.CreateMetricsReport(model = loaded_model,\n",
    "                             dataset = custom_test_ds,\n",
    "                             confusion_matrix = confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba2b759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
